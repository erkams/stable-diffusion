model:
  base_learning_rate: 1.0e-04
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    linear_start: 0.00085
    linear_end: 0.0120
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: "image"
    cond_stage_key: "txt"
    image_size: 64
    channels: 4
    cond_stage_trainable: false   # Note: different from the one we trained before
    conditioning_key: crossattn
    scale_factor: 0.18215

    scheduler_config: # 10000 warmup steps
      target: ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps: [ 1 ] # NOTE for resuming. use 10000 if starting from scratch
        cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases
        f_start: [ 1.e-6 ]
        f_max: [ 1. ]
        f_min: [ 1. ]

    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 32 # unused
        in_channels: 4
        out_channels: 4
        model_channels: 320
        attention_resolutions: [ 4, 2, 1 ]
        num_res_blocks: 2
        channel_mult: [ 1, 2, 4, 4 ]
        num_heads: 8
        use_spatial_transformer: True
        transformer_depth: 1
        context_dim: 768
        use_checkpoint: True
        legacy: False

    first_stage_config:
      target: ldm.models.autoencoder.AutoencoderKL
      ckpt_path: "models/first_stage_models/kl-f8/model.ckpt"
      params:
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity

    cond_stage_config:
      target: ldm.modules.encoders.modules.FrozenCLIPEmbedder


data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 4
    num_workers: 4
    num_val_workers: 0 # Avoid a weird val dataloader issue
    train:
      target: ldm.data.simple.hf_dataset
      params:
        name: visual_genome
        dir: relationships_v1.0.0
        image_transforms:
        - target: torchvision.transforms.Resize
          params:
            size: 512
            interpolation: 3
        - target: torchvision.transforms.RandomCrop
          params:
            size: 512
        - target: torchvision.transforms.RandomHorizontalFlip
    validation:
      target: ldm.data.simple.TextOnly
      params:
        captions:
        - "shade on street.\nman wears sneakers.\ncar has headlight.\nsign on building.\ntree trunk on sidewalk.\nman has shirt.\nsidewalk next to street.\ncar has back.\nman has glasses.\nparking meter on sidewalk.\nman wears sneakers.\nman has shoes.\nman has shirt.\nman wears pants.\nman has jacket.\nman has pants.\nbike parked on sidewalk.\nwork truck parked on street.\ncar parked on street.\nparking meter behind man.\nguy holding chin.\nman wearing shirt.\nman holding chin.\nbikes near tree.\nman wearing shoes.\nbikes near tree.\nshirt on man.\nman in shirt.\nman wearing pants.\nparking meter on top of street.\ntree next to street.\nman wearing glasses.\nbikes behind man.\ntrees by sidewalk.\nman wearing jacket.\n"
        - "man wears backpack.\ncar parked on road.\ntree next to road.\ncrosswalk in front of man.\nbuilding has window.\ncar parked on road.\nsign next to road.\ntree along road.\nman wears sneakers.\nman carries backpack.\nbike next to car.\ncar parked on road.\nman at crosswalk.\ncrosswalk on road.\nwalk sign says "walk".\n"
        - "curtains have curtains.\nchair has padding.\ncarpet on floor.\npillow on couch.\nteddy bear against pillow.\nanimal against pillow.\ntable top near window.\nfuton has frame.\nchair next to table.\ndoor to patio.\ndoll has cloths.\nlamp behind the couch.\npillow on the couch.\nchair by the table top.\nteddy bear on the couch.\ntable top on the table.\nlamp on the floor.\n"
        - "chair is at the desk.\nthe computer is on the desk.\nthe box is under the table.\na building is seen from the window.\nthe mug is on the desk.\nthe carton is under the table.\nthe cpu is below the table.\nthe mouse is on the table.\nthe bowl is on the table.\nthe pot has flowers.\nthe clothing is on the table.\nthe chair has five legs.\nthe cup is on the table.\nmouse on desk.\nplate on the desk.\nthe plant on desk.\nthe cup on the desk.\nthe box under the desk.\nthe mouse on the mouse pad.\nbowl on desk.\ncomputer tower on floor.\nlint on floor.\nbag on the desk.\nchair by desk.\nthe mug on the desk.\nchair with arm rest.\ncomputer on the desk.\nmonitor with keyboard.\n"
        output_size: 512
        n_gpus: 2 # small hack to sure we see all our samples


lightning:
  find_unused_parameters: False

  modelcheckpoint:
    params:
      every_n_train_steps: 2000
      save_top_k: -1
      monitor: null

  callbacks:
    image_logger:
      target: main.ImageLogger
      params:
        batch_frequency: 2000
        max_images: 4
        increase_log_steps: False
        log_first_step: True
        log_all_val: True
        log_images_kwargs:
          use_ema_scope: True
          inpaint: False
          plot_progressive_rows: False
          plot_diffusion_rows: False
          N: 4
          unconditional_guidance_scale: 3.0
          unconditional_guidance_label: [""]

  trainer:
    benchmark: True
    num_sanity_val_steps: 0
    accumulate_grad_batches: 1
